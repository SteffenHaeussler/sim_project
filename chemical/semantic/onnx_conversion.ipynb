{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73876406-52da-48fc-880e-46b7427cb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModelForFeatureExtraction\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2fa1db4-36f9-4b57-984f-4acf736ecc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model cross-encoder/ms-marco-MiniLM-L12-v2 was already converted to ONNX but got `export=True`, the model will be converted to ONNX once again. Don't forget to save the resulting model with `.save_pretrained()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ms-marco-MiniLM-L12-v2/tokenizer_config.json',\n",
       " 'ms-marco-MiniLM-L12-v2/special_tokens_map.json',\n",
       " 'ms-marco-MiniLM-L12-v2/vocab.txt',\n",
       " 'ms-marco-MiniLM-L12-v2/added_tokens.json',\n",
       " 'ms-marco-MiniLM-L12-v2/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if ONNX model already exists to save time\n",
    "onnx_model_dir = \"ms-marco-MiniLM-L12-v2\"\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_name,          \n",
    "    export=True,         \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "ort_model.save_pretrained(onnx_model_dir)\n",
    "tokenizer.save_pretrained(onnx_model_dir) # Save tokenizer for easy loading with ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae86bf9-3fe2-4a1f-8f2f-4b1febc3c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = [\n",
    "    (\"How is the weather today?\", \"The weather is sunny and warm.\"),\n",
    "    (\"What is ONNX Runtime?\", \"It is a cross-platform inferencing and training accelerator.\"),\n",
    "    (\"This is a relevant document.\", \"This is a relevant document.\"),\n",
    "    (\"This is a relevant document.\", \"This is a completely irrelevant document.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41a3527f-7f18-4b6c-9240-d6d28a71c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference with Optimum ORTModel ---\n",
      "Input pairs: [('How is the weather today?', 'The weather is sunny and warm.'), ('What is ONNX Runtime?', 'It is a cross-platform inferencing and training accelerator.'), ('This is a relevant document.', 'This is a relevant document.'), ('This is a relevant document.', 'This is a completely irrelevant document.')]\n",
      "Logits from Optimum ORTModel:\n",
      "  Pair 1: Score = 0.7604\n",
      "  Pair 2: Score = -8.9707\n",
      "  Pair 3: Score = 7.9912\n",
      "  Pair 4: Score = 1.2184\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running Inference with Optimum ORTModel ---\")\n",
    "ort_model_loaded = ORTModelForSequenceClassification.from_pretrained(\n",
    "    onnx_model_dir, \n",
    ")\n",
    "tokenizer_for_ort = AutoTokenizer.from_pretrained(onnx_model_dir)\n",
    "\n",
    "# Prepare input (Optimum can handle PyTorch tensors or NumPy arrays)\n",
    "encoded_input_optimum = tokenizer_for_ort(\n",
    "    test_pairs,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\" # Can be \"pt\" for PyTorch tensors or \"np\" for NumPy\n",
    ")\n",
    "\n",
    "outputs_optimum = ort_model_loaded(**encoded_input_optimum)\n",
    "\n",
    "logits_optimum = outputs_optimum.logits # Get logits and move to CPU if they were on GPU\n",
    "\n",
    "print(\"Input pairs:\", test_pairs)\n",
    "print(\"Logits from Optimum ORTModel:\")\n",
    "for i, pair in enumerate(test_pairs):\n",
    "    print(f\"  Pair {i+1}: Score = {logits_optimum[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e54fd054-a88d-49ac-9e8a-bf405276eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.8144011e-01]\n",
      " [1.2706606e-04]\n",
      " [9.9966168e-01]\n",
      " [7.7177614e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "print(sigmoid(logits_optimum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608890fb-2629-4f91-b930-f9b5f90f216a",
   "metadata": {},
   "source": [
    "## BiEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9d79259-832a-4982-8b06-cc65faaa970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # HF model name\n",
    "onnx_model_optimum_dir = \"all-MiniLM-L6-v2\" # Directory for Optimum's ONNX export\n",
    "\n",
    "# Load original PyTorch model (base) and tokenizer\n",
    "# For sentence-transformers, the AutoModel gives the base transformer\n",
    "model_base = AutoModel.from_pretrained(model_name).eval()\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f8d3cf-0623-45b5-808b-474fe9d1ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model sentence-transformers/all-MiniLM-L6-v2 was already converted to ONNX but got `export=True`, the model will be converted to ONNX once again. Don't forget to save the resulting model with `.save_pretrained()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('all-MiniLM-L6-v2/tokenizer_config.json',\n",
       " 'all-MiniLM-L6-v2/special_tokens_map.json',\n",
       " 'all-MiniLM-L6-v2/vocab.txt',\n",
       " 'all-MiniLM-L6-v2/added_tokens.json',\n",
       " 'all-MiniLM-L6-v2/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_model_exporter = ORTModelForFeatureExtraction.from_pretrained(\n",
    "        model_name,\n",
    "        export=True,\n",
    "        # provider=\"CUDAExecutionProvider\" # if you want to optimize for CUDA during export\n",
    "    )\n",
    "ort_model_exporter.save_pretrained(onnx_model_optimum_dir)\n",
    "tokenizer_base.save_pretrained(onnx_model_optimum_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e886e844-5064-4975-a3d9-580bdc7966ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"Each sentence is converted to a vector.\",\n",
    "    \"ONNX makes deployment easier.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6a56000-35b6-4c45-8e97-b5191ce8e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference with Optimum ORTModel ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running Inference with Optimum ORTModel ---\")\n",
    "ort_model_loaded = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    onnx_model_optimum_dir, \n",
    ")\n",
    "tokenizer_for_ort = AutoTokenizer.from_pretrained(onnx_model_optimum_dir)\n",
    "\n",
    "# Prepare input (Optimum can handle PyTorch tensors or NumPy arrays)\n",
    "encoded_input_optimum = tokenizer_for_ort(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\" # Can be \"pt\" for PyTorch tensors or \"np\" for NumPy\n",
    ")\n",
    "\n",
    "outputs_optimum = ort_model_loaded(**encoded_input_optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3daeb76a-9cb9-45b0-aa77-ab4cf36ecacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_embedding = np.mean(outputs_optimum.last_hidden_state,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "273125fd-39cf-4b8a-a1ec-20f2112d44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings_numpy(embeddings_np, p=2, axis=1, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Performs L2 normalization on embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings_np (np.ndarray): Embeddings to normalize, e.g., (batch_size, hidden_dim).\n",
    "        p (int, optional): The order of the norm. Defaults to 2 (L2 norm).\n",
    "        axis (int, optional): The axis along which to compute the norm. Defaults to 1.\n",
    "        epsilon (float, optional): A small value to add to the norm to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized embeddings.\n",
    "    \"\"\"\n",
    "    # Calculate the norm\n",
    "    norm = np.linalg.norm(embeddings_np, ord=p, axis=axis, keepdims=True)\n",
    "\n",
    "    # Add epsilon to prevent division by zero if norm is 0\n",
    "    norm = np.maximum(norm, epsilon)\n",
    "\n",
    "    return embeddings_np / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62838142-c6b5-4e1f-8599-3c137d727932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
